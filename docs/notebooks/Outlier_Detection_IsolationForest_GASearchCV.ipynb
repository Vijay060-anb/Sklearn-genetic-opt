# Minimal example: IsolationForest tuned by GASearchCV (unsupervised)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
from sklearn.ensemble import IsolationForest

from sklearn_genetic import GASearchCV
from sklearn_genetic.space import Integer, Continuous
from sklearn_genetic.plots import plot_fitness_evolution


# Synthetic data with injected outliers
rng = np.random.RandomState(42)
X_normal, _ = make_blobs(n_samples=600, centers=1, n_features=4, cluster_std=0.7, random_state=42)
X_out = rng.uniform(-6, 6, size=(60, 4))
X = np.vstack([X_normal, X_out])
y = np.hstack([np.zeros(len(X_normal), int), np.ones(len(X_out), int)])  # 1 = outlier

# GA search over IsolationForest (unsupervised -> scoring=None)
est = IsolationForest(random_state=42)
param_grid = {
    "n_estimators": Integer(50, 300),
    "max_samples": Continuous(0.5, 1.0),
    "contamination": Continuous(0.05, 0.25),
}
ga = GASearchCV(
    estimator=est,
    param_grid=param_grid,
    scoring=None,        # uses the packageâ€™s default for outlier models
    cv=3,
    generations=6,
    population_size=12,
    n_jobs=-1,
    verbose=True,
    random_state=42,
)
ga.fit(X)
print("Best params:", ga.best_params_)

# Fitness evolution plot
plot_fitness_evolution(ga)
plt.show()

# Optional quick evaluation using synthetic labels
best_if = ga.best_estimator_
scores = -best_if.decision_function(X)  # higher = more anomalous
roc = roc_auc_score(y, scores)
k = int(y.sum())
th = np.partition(scores, -k)[-k]
y_pred = (scores >= th).astype(int)
prec, rec, f1, _ = precision_recall_fscore_support(y, y_pred, average="binary")
print({"roc_auc": roc, "precision": prec, "recall": rec, "f1": f1})
